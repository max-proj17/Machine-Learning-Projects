{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pickle\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "Load the MNIST dataset. Split this dataset into training (70%), validation (10%), and test dataset\n",
    "(20%).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maxfi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load MNIST data\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "y = y.astype(int)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(2/3), random_state=1)\n",
    "\n",
    "# Add bias term to the features\n",
    "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "X_val = np.hstack([np.ones((X_val.shape[0], 1)), X_val])\n",
    "X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "\n",
    "# Number of classes\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Testing Function with Covariance\n",
    "def testGaussianMCClassifier(X, y, A, cov_inv):\n",
    "    scores = np.zeros((X.shape[0], n_classes))\n",
    "    for i in range(n_classes):\n",
    "        diff = X - A[:, i]\n",
    "        scores[:, i] = -0.5 * np.sum(diff @ cov_inv[:, :, i] * diff, axis=1)\n",
    "    y_pred = np.argmax(scores, axis=1)\n",
    "    misclassifications = np.sum(y_pred != y)\n",
    "    return misclassifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "Consider the multi-class Gaussian classifier in page 23 of Logistic regression slide deck. Assume a\n",
    "common covariance matrix Σ = I. Estimate the parameters μi; i = 0, .., n − 1 from the training\n",
    "dataset. Develop a python function called gaussianMultiChannelClassifier that implements the\n",
    "above classifier. This function accepts the training dataset and the labels, and returns the matrix A1,\n",
    "whose columns are ai that corresponds to discriminants yi(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dimensionality Reduction with PCA\n",
    "pca = PCA(n_components=0.95)  # retain 95% variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_val_pca = pca.transform(X_val_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Gaussian Multi-Channel Classifier with Covariance\n",
    "def gaussianMultiChannelClassifierPCA(Xtrain, ytrain, reg_lambda=0.8):\n",
    "    A = np.zeros((Xtrain.shape[1], n_classes))\n",
    "    cov_inv = np.zeros((Xtrain.shape[1], Xtrain.shape[1], n_classes))\n",
    "    for i in range(n_classes):\n",
    "        X_i = Xtrain[ytrain == i]\n",
    "        mu_i = np.mean(X_i, axis=0)\n",
    "        cov_i = np.cov(X_i, rowvar=False) + reg_lambda*np.identity(Xtrain.shape[1])\n",
    "        cov_inv[:, :, i] = np.linalg.inv(cov_i)\n",
    "        A[:, i] = np.dot(cov_inv[:, :, i], mu_i)\n",
    "    return A, cov_inv\n",
    "\n",
    "def find_best_lambda(X_train, y_train, X_val, y_val, lambda_values):\n",
    "    best_lambda = None\n",
    "    lowest_error = float('inf')\n",
    "\n",
    "    for lambda_val in lambda_values:\n",
    "        # Train the classifier with the current lambda value\n",
    "        A, cov_inv = gaussianMultiChannelClassifier(X_train, y_train, reg_lambda=lambda_val)\n",
    "        \n",
    "        # Evaluate on the validation set\n",
    "        misclassifications = testGaussianMCClassifier(X_val, y_val, A, cov_inv)\n",
    "        percent_error = (misclassifications / X_val.shape[0]) * 100\n",
    "\n",
    "        # Check if this is the best lambda so far\n",
    "        if percent_error < lowest_error:\n",
    "            best_lambda = lambda_val\n",
    "            lowest_error = percent_error\n",
    "\n",
    "        print(f\"Lambda: {lambda_val}, Validation Error: {percent_error:.2f}%\")\n",
    "\n",
    "    return best_lambda\n",
    "\n",
    "# \n",
    "# Fine tune stage 1\n",
    "# lambda_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Fine tune stage 2\n",
    "#lambda_values = [0.5, 0.8, 0.9, 1, 1.1, 1.2, 1.5]\n",
    "\n",
    "# Fine tine stage 3\n",
    "# I created an array of lambda values evenly spaced between 0.8 and 1\n",
    "#lambda_values = np.linspace(0.8, 1, 100)\n",
    "\n",
    "# Find the best lambda\n",
    "# best_lambda = find_best_lambda(X_train_scaled, y_train, X_val_scaled, y_val, lambda_values)\n",
    "# best_lambda = find_best_lambda(X_train_scaled, y_train, X_val_scaled, y_val, lambda_values)\n",
    "# print(f\"Best Lambda: {best_lambda}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "implement a multiclass logistic regression algorithm to this dataset. Develop a python function called\n",
    "logisticRegressionMultiClassClassifier that implements the above classifier. This function ac-\n",
    "cepts the training dataset and the labels, and returns the matrix A2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Multiclass Logistic Regression Classifier\n",
    "def logisticRegressionMultiClassClassifier(Xtrain, ytrain, iterations=500, learning_rate=1e-4):\n",
    "    A = np.zeros((Xtrain.shape[1], n_classes))\n",
    "    for i in range(iterations):\n",
    "        scores = np.dot(Xtrain, A)\n",
    "        probs = softmax(scores, axis=1)\n",
    "        y_one_hot = np.eye(n_classes)[ytrain]\n",
    "        gradient = np.dot(Xtrain.T, (probs - y_one_hot)) / Xtrain.shape[0]\n",
    "        A -= learning_rate * gradient\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "implement a regularized version of multi-class logistic regression. Develop a python function called\n",
    "logisticRegressionMultiClassClassifierWithRegularization that implements the above classi-\n",
    "fier. This function accept the training dataset, labels, and λ and return the matrix A3. You may test\n",
    "this function independently with a specific λ value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Regularized Multiclass Logistic Regression\n",
    "def logisticRegressionMultiClassClassifierWithRegularization(Xtrain, ytrain, lam=0.1, iterations=500, learning_rate=1e-4):\n",
    "    A = np.zeros((Xtrain.shape[1], n_classes))\n",
    "    for i in range(iterations):\n",
    "        scores = np.dot(Xtrain, A)\n",
    "        probs = softmax(scores, axis=1)\n",
    "        y_one_hot = np.eye(n_classes)[ytrain]\n",
    "        gradient = np.dot(Xtrain.T, (probs - y_one_hot)) / Xtrain.shape[0] + lam * A\n",
    "        A -= learning_rate * gradient\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 \n",
    "Write a function testLinearMCClassifier to test the performance of a multiclass classifier specified\n",
    "by its weights A. This function would accept the test dataset, labels, and A and would return the\n",
    "number of misclassified points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Testing Functions\n",
    "def testLinearMCClassifier(X, y, A):\n",
    "    scores = np.dot(X, A)\n",
    "    y_pred = np.argmax(scores, axis=1)\n",
    "    misclassifications = np.sum(y_pred != y)\n",
    "    return misclassifications\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "Write a function Optimize_MC_Hyperparameters to determine the optimal λ of\n",
    "logisticRegressionMultiClassClassifierWithRegularization. This function accepts the train-\n",
    "ing dataset, validation dataset, and the labels, and returns the matrix (A_1)that corresponds to the\n",
    "optimal logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Optimize Hyperparameters\n",
    "\n",
    "# Multithreaded Lambda evaluation\n",
    "def train_and_evaluate_lambda(Xtrain, ytrain, Xval, yval, lam, iterations, learning_rate):\n",
    "    A = logisticRegressionMultiClassClassifierWithRegularization(Xtrain, ytrain, lam, iterations, learning_rate)\n",
    "    error = testLinearMCClassifier(Xval, yval, A)\n",
    "    return lam, error, A\n",
    "\n",
    "def Optimize_MC_Hyperparameters(Xtrain, ytrain, Xval, yval, iterations=500, learning_rate=1e-4):\n",
    "    results = Parallel(n_jobs=-1)(delayed(train_and_evaluate_lambda)(\n",
    "        Xtrain, ytrain, Xval, yval, lam, iterations, learning_rate) for lam in np.logspace(-4, 2, 20))\n",
    "    \n",
    "    best_result = min(results, key=lambda x: x[1])\n",
    "    print(f'Best lambda: {best_result[0]}')\n",
    "    return best_result[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7\n",
    "\n",
    "Write a script that would compare the performance all the above algorithms on the test subset. This\n",
    "script would call testLinearMCClassifier with each A1, A2 and report the misclassifications\n",
    "in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda: 0.007847599703514606\n",
      "Gaussian Multi-Channel Classifier Percentage Error: 8.44%\n",
      "Multiclass Logistic Regression Percentage Error: 8.39%\n",
      "Optimized Multiclass Logistic Regression Percentage Error: 8.26%\n"
     ]
    }
   ],
   "source": [
    "# 7. Comparison Script\n",
    "A1, cov_inv_A1 = gaussianMultiChannelClassifierPCA(X_train_pca, y_train)\n",
    "A2 = logisticRegressionMultiClassClassifier(X_train, y_train, iterations=1000)\n",
    "A3 = Optimize_MC_Hyperparameters(X_train, y_train, X_val, y_val, iterations=1000)\n",
    "\n",
    "# Test classifiers (missclassifications)\n",
    "misclassifications_A1 = testGaussianMCClassifier(X_test_pca, y_test, A1, cov_inv_A1)\n",
    "misclassifications_A2 = testLinearMCClassifier(X_test, y_test, A2)\n",
    "misclassifications_A3 = testLinearMCClassifier(X_test, y_test, A3)\n",
    "\n",
    "# Calculate percentage errors\n",
    "percent_error_A1 = (misclassifications_A1 / X_test_pca.shape[0]) * 100\n",
    "percent_error_A2 = (misclassifications_A2 / X_test.shape[0]) * 100\n",
    "percent_error_A3 = (misclassifications_A3 / X_test.shape[0]) * 100\n",
    "\n",
    "# Print out the percentage errors\n",
    "print(f\"Gaussian Multi-Channel Classifier Percentage Error: {percent_error_A1:.2f}%\")\n",
    "print(f\"Multiclass Logistic Regression Percentage Error: {percent_error_A2:.2f}%\")\n",
    "print(f\"Optimized Multiclass Logistic Regression Percentage Error: {percent_error_A3:.2f}%\")\n",
    "\n",
    "# Example Output\n",
    "# Best lambda: 0.007847599703514606\n",
    "# Gaussian Multi-Channel Classifier Percentage Error: 8.44%\n",
    "# Multiclass Logistic Regression Percentage Error: 8.39%\n",
    "# Optimized Multiclass Logistic Regression Percentage Error: 8.26%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
