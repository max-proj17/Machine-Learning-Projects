{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8y-8qd8g4Lj",
        "outputId": "5dcacfb3-c74e-4909-99cf-dc9c42c6f738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install matplotlib\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "class NerfModel(nn.Module):\n",
        "    def __init__(self, embedding_dim_pos=10, embedding_dim_direction=4, hidden_dim=128):\n",
        "        super(NerfModel, self).__init__()\n",
        "\n",
        "        # The positional encoding of the input location is passed through 8 fully-connected ReLU layers.\n",
        "        # The input to the first block is the position encoded vector concatenated with the raw 3D coordinates.\n",
        "        self.block1 = nn.Sequential(nn.Linear(embedding_dim_pos * 6 + 3, hidden_dim), nn.ReLU(),\n",
        "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
        "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
        "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), )\n",
        "        # A skip connection from the input to the fifth layer’s activation.\n",
        "        # Outputs the volume density (sigma) and a feature vector.\n",
        "        self.block2 = nn.Sequential(nn.Linear(embedding_dim_pos * 6 + hidden_dim + 3, hidden_dim), nn.ReLU(),\n",
        "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
        "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
        "                                    nn.Linear(hidden_dim, hidden_dim + 1), )\n",
        "        # The feature vector is concatenated with the positional encoding of the input viewing direction.\n",
        "        # This is processed by an additional fully-connected ReLU layer with 128 channels.\n",
        "        self.block3 = nn.Sequential(nn.Linear(embedding_dim_direction * 6 + hidden_dim + 3, hidden_dim // 2), nn.ReLU(), )\n",
        "        self.block4 = nn.Sequential(nn.Linear(hidden_dim // 2, 3), nn.Sigmoid(), )\n",
        "\n",
        "        # Store the dimensions of the embeddings for access during forward pass.\n",
        "        self.embedding_dim_pos = embedding_dim_pos\n",
        "        self.embedding_dim_direction = embedding_dim_direction\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    # Positional encoding helps the model to understand the relative or absolute position of the data points.\n",
        "    @staticmethod\n",
        "    def positional_encoding(x, L):\n",
        "        out = [x]\n",
        "        for j in range(L):\n",
        "            out.append(torch.sin(2 ** j * x))\n",
        "            out.append(torch.cos(2 ** j * x))\n",
        "        return torch.cat(out, dim=1)\n",
        "    # The forward pass of the model, where the actual computation on the input happens.\n",
        "    # It takes object positions 'o' and directions 'd', processes them through the network, and returns color 'c' and density 'sigma'.\n",
        "    def forward(self, o, d):\n",
        "        emb_x = self.positional_encoding(o, self.embedding_dim_pos)\n",
        "        emb_d = self.positional_encoding(d, self.embedding_dim_direction)\n",
        "        h = self.block1(emb_x)\n",
        "        tmp = self.block2(torch.cat((h, emb_x), dim=1))\n",
        "        h, sigma = tmp[:, :-1], self.relu(tmp[:, -1])\n",
        "        h = self.block3(torch.cat((h, emb_d), dim=1))\n",
        "        c = self.block4(h)\n",
        "        return c, sigma\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(hn, hf, dataset, chunk_size=10, img_index=0, nb_bins=192, H=400, W=400):\n",
        "\n",
        "    ray_origins = dataset[img_index * H * W: (img_index + 1) * H * W, :3]\n",
        "    ray_directions = dataset[img_index * H * W: (img_index + 1) * H * W, 3:6]\n",
        "\n",
        "    data = []\n",
        "    for i in range(int(np.ceil(H / chunk_size))):\n",
        "        ray_origins_ = ray_origins[i * W * chunk_size: (i + 1) * W * chunk_size].to(device)\n",
        "        ray_directions_ = ray_directions[i * W * chunk_size: (i + 1) * W * chunk_size].to(device)\n",
        "        regenerated_px_values = render_rays(model, ray_origins_, ray_directions_, hn=hn, hf=hf, nb_bins=nb_bins)\n",
        "        data.append(regenerated_px_values)\n",
        "    img = torch.cat(data).data.cpu().numpy().reshape(H, W, 3)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(img)\n",
        "    plt.savefig(f'/content/drive/MyDrive/Colab Notebooks/Nerf_Imp/results/img_{img_index}.png', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# Compute the accumulated transmittance, which represents how much light is not blocked by the medium.\n",
        "# This is essential for volume rendering, where you accumulate color along a ray through a semi-transparent medium.\n",
        "def compute_accumulated_transmittance(alphas):\n",
        "    # Cumulative product of transmittance across sampled points along the ray.\n",
        "    # This is necessary to calculate how much light reaches each point.\n",
        "    accumulated_transmittance = torch.cumprod(alphas, 1)\n",
        "    return torch.cat((torch.ones((accumulated_transmittance.shape[0], 1), device=alphas.device),\n",
        "                      accumulated_transmittance[:, :-1]), dim=-1)\n",
        "\n",
        "\n",
        "# Render the rays to generate the final image.\n",
        "# This function takes in ray origins and directions, as well as near and far bounds and number of bins to sample along the ray.\n",
        "def render_rays(nerf_model, ray_origins, ray_directions, hn=0, hf=0.5, nb_bins=192):\n",
        "    device = ray_origins.device\n",
        "\n",
        "    t = torch.linspace(hn, hf, nb_bins, device=device).expand(ray_origins.shape[0], nb_bins)\n",
        "    # Sample points along each ray for integral approximation.\n",
        "    mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
        "    lower = torch.cat((t[:, :1], mid), -1)\n",
        "    upper = torch.cat((mid, t[:, -1:]), -1)\n",
        "    u = torch.rand(t.shape, device=device)\n",
        "    t = lower + (upper - lower) * u\n",
        "\n",
        "    delta = torch.cat((t[:, 1:] - t[:, :-1], torch.tensor([1e10], device=device).expand(ray_origins.shape[0], 1)), -1)\n",
        "    # Calculate differences between adjacent samples to compute differential transmittance.\n",
        "\n",
        "    x = ray_origins.unsqueeze(1) + t.unsqueeze(2) * ray_directions.unsqueeze(1)   # Calculate 3D points along the rays\n",
        "\n",
        "    ray_directions = ray_directions.expand(nb_bins, ray_directions.shape[0], 3).transpose(0, 1)\n",
        "\n",
        "    colors, sigma = nerf_model(x.reshape(-1, 3), ray_directions.reshape(-1, 3))   # Get colors and densities at sample points\n",
        "    colors = colors.reshape(x.shape) # Reshape to match the input ray shapes\n",
        "    sigma = sigma.reshape(x.shape[:-1]) # Reshape to match the input ray shapes, sans the color dimension\n",
        "\n",
        "    # Compute the opacity for each sample point along the ray based on the volume density (sigma) and the distance between samples (delta).\n",
        "    alpha = 1 - torch.exp(-sigma * delta)\n",
        "\n",
        "    # Calculate the weights for each sample point. This is done by computing the accumulated transmittance (which accounts for the\n",
        "    # amount of light not blocked by preceding sample points) and then multiplying by the alpha value for each point.\n",
        "    # This effectively gives us a weight that represents the contribution of each point's color to the final pixel color.\n",
        "    weights = compute_accumulated_transmittance(1 - alpha).unsqueeze(2) * alpha.unsqueeze(2)\n",
        "\n",
        "    # Sum the weighted colors along each ray to get the final color for each pixel. The weights ensure that colors from points\n",
        "   # closer to the camera or in denser parts of the volume have a bigger impact on the final color.\n",
        "    c = (weights * colors).sum(dim=1)\n",
        "    # Compute the sum of the weights for each ray. This is used to adjust the final color based on the amount of light that\n",
        "    # reaches the camera without being absorbed or scattered. If all light is absorbed, weight_sum will be close to 0; if\n",
        "    # all light passes through, weight_sum will be close to 1.\n",
        "    weight_sum = weights.sum(-1).sum(-1)\n",
        "\n",
        "    # Return the final color for each ray. The expression \"c + 1 - weight_sum.unsqueeze(-1)\" adjusts the final color by adding\n",
        "    # a small amount of ambient light to ensure that completely unlit areas have a minimal brightness instead of being pitch black.\n",
        "    # This can help prevent areas that are fully occluded or in shadow from being entirely dark, improving visual quality.\n",
        "    return c + 1 - weight_sum.unsqueeze(-1)\n",
        "\n",
        "\n",
        "def train(nerf_model, optimizer, scheduler, data_loader, device='cpu', hn=0, hf=1, nb_epochs=int(1e5),\n",
        "          nb_bins=192, H=400, W=400):\n",
        "    training_loss = []\n",
        "    for _ in tqdm(range(nb_epochs)):\n",
        "        for batch in data_loader:\n",
        "            ray_origins = batch[:, :3].to(device)\n",
        "            ray_directions = batch[:, 3:6].to(device)\n",
        "            ground_truth_px_values = batch[:, 6:].to(device)\n",
        "\n",
        "            regenerated_px_values = render_rays(nerf_model, ray_origins, ray_directions, hn=hn, hf=hf, nb_bins=nb_bins)\n",
        "            loss = ((ground_truth_px_values - regenerated_px_values) ** 2).sum()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            training_loss.append(loss.item())\n",
        "        scheduler.step()\n",
        "\n",
        "        for img_index in range(200):\n",
        "            test(hn, hf, testing_dataset, img_index=img_index, nb_bins=nb_bins, H=H, W=W)\n",
        "    return training_loss\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    device = 'cuda'\n",
        "\n",
        "    training_dataset = torch.from_numpy(np.load('/content/drive/MyDrive/Colab Notebooks/Nerf_Imp/training_data.pkl', allow_pickle=True))\n",
        "    testing_dataset = torch.from_numpy(np.load('/content/drive/MyDrive/Colab Notebooks/Nerf_Imp/testing_data.pkl', allow_pickle=True))\n",
        "    model = NerfModel(hidden_dim=256).to(device)\n",
        "    model_optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(model_optimizer, milestones=[2, 4, 8], gamma=0.5)\n",
        "    data_loader = DataLoader(training_dataset, batch_size=1024, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    train(model, model_optimizer, scheduler, data_loader, nb_epochs=16, device=device, hn=2, hf=6, nb_bins=192, H=400,\n",
        "          W=400)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvtabK3nhPu4",
        "outputId": "1654ae4c-a02b-4a1d-e25c-33beee3ebe44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▍     | 7/16 [4:33:39<6:09:00, 2460.06s/it]"
          ]
        }
      ]
    }
  ]
}